{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90b388-2eb3-49aa-a0c0-9b7639e66079",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget \n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "from filelock import FileLock\n",
    "import tap\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as transforms_f\n",
    "from rlbench.observation_config import ObservationConfig, CameraConfig\n",
    "from rlbench.environment import Environment\n",
    "from rlbench.backend.observation import Observation\n",
    "from rlbench.task_environment import TaskEnvironment\n",
    "from rlbench.action_modes.action_mode import MoveArmThenGripper\n",
    "from rlbench.action_modes.gripper_action_modes import Discrete\n",
    "from rlbench.action_modes.arm_action_modes import EndEffectorPoseViaPlanning\n",
    "from rlbench.backend.exceptions import InvalidActionError\n",
    "from PIL import Image\n",
    "from create_data import RLBench\n",
    "import ipdb\n",
    "import open3d as o3d\n",
    "from open3d.web_visualizer import draw\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from rotations import compute_rotation_matrix_from_quaternions\n",
    "from network import PlainUNet, TransformerUNet\n",
    "from utils import (\n",
    "    RLBenchEnv,\n",
    "    task_file_to_task_class,\n",
    "    count_parameters,\n",
    "    load_episodes,\n",
    "    load_instructions\n",
    ")\n",
    "from data_gen_sim2real import Arguments as S2RArguments, load_demo, Demo, get_obs_action_from_demo\n",
    "from utils_sim2real import get_gripper_state, depth_to_pcd, get_extrinsics, get_intrinsics, square, get_rgb_pcd_gripper_from_obs,get_extrinsics,get_intrinsics\n",
    "\n",
    "\n",
    "task = 'put_money_in_safe'\n",
    "sys.argv = ['foo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777cbff2-949e-48ec-959b-4db52fc1a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RLBench('dataset', task)\n",
    "frame_id, state, action, padding_mask = dataset[5]\n",
    "print(frame_id, state.shape, action.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064977e-46ef-472e-be7f-04d48fd0a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, batch_size=32//5, shuffle=True, num_workers=0\n",
    ")\n",
    "train_dataset = iter(train_loader)\n",
    "device = 'cpu'\n",
    "\n",
    "for i in trange(8000):\n",
    "    try:\n",
    "        frame_id, state, action, padding_mask = train_dataset.next()\n",
    "    except:\n",
    "        train_dataset = iter(train_loader)\n",
    "        frame_id, state, action, padding_mask = train_dataset.next()\n",
    "\n",
    "    inputs = state.to(device)\n",
    "    outputs = action.to(device)\n",
    "\n",
    "    rgb, pc = torch.split(\n",
    "        inputs, [inputs.shape[2] // 2, inputs.shape[2] // 2], dim=2\n",
    "    )\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d0e88-6685-49e6-83d5-173c6ef0166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [task]\n",
    "episodes = load_episodes()\n",
    "max_eps_list = {task_str: episodes[task_str] for task_str in tasks}\n",
    "\n",
    "t_list: Dict[str, torch.Tensor] = {}\n",
    "z_list: Dict[str, torch.Tensor] = {}\n",
    "for task_str, max_eps in max_eps_list.items():\n",
    "    t_list[task_str] = torch.rand(max_eps, 64, requires_grad=True, device=device)\n",
    "    z_list[task_str] = torch.zeros(max_eps, 3, requires_grad=True, device=device)\n",
    "\n",
    "# model = PlainUNet(action_size=8, depth=4, temp_len=64).to(device)\n",
    "model = TransformerUNet(action_size=8, depth=4, temp_len=64).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d23e80-efa4-4d5e-8ead-85802696fee0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb440632-d4ed-4724-9dcf-34b1673ddab2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Raw RLBench sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ad612-980f-410f-bc3e-39e2eb85f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import RLBenchEnv\n",
    "from data_gen import Arguments as DataArguments\n",
    "args = DataArguments().parse_args()\n",
    "args.data_dir = Path('real_raw')\n",
    "env = RLBenchEnv(\n",
    "        data_path=args.data_dir,\n",
    "        apply_rgb=True,\n",
    "        apply_pc=True,\n",
    "        apply_cameras=args.cameras,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb50890-0a0b-430d-af1d-516d9a045634",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    #\"stack_wine\", \"stack_blocks\", \"sweep_to_dustpan\", \n",
    "    # \"pick_up_cup\",\n",
    "    #\"toilet_seat_down\", \"close_fridge\", \n",
    "    #\"/scratch/e_usb_out_of_computer\", \n",
    "    # \"water_plants\",\n",
    "    #\"change_clock\", \n",
    "    #\"insert_into_square_peg\",\n",
    "    #\"take_lid_off_saucepan\"\n",
    "    \"push_buttons\"\n",
    "]\n",
    "\n",
    "for task in tasks:\n",
    "    demo = env.get_demo(task, 4, i)\n",
    "    key_frame = keypoint_discovery(demo)\n",
    "    print(task, key_frame)\n",
    "    #key_frame = [k for i, k in enumerate(key_frame) if i % 6 in set([1, 4])]\n",
    "    #for i, kp in enumerate(key_frame):\n",
    "    #    plt.subplot(131)\n",
    "    #    plt.imshow(demo[kp].left_shoulder_rgb)\n",
    "    #   plt.subplot(132)\n",
    "    #    plt.imshow(demo[kp].right_shoulder_rgb)\n",
    "    #    plt.subplot(133)\n",
    "    #    plt.imshow(demo[kp].wrist_rgb)\n",
    "    #    plt.title(f'{kp} {i}')\n",
    "    #    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40960ada-fdea-457e-b408-a4cf1e6a0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "     \"insert_onto_square_peg\",\n",
    "    \"stack_wine\", \n",
    "    \"stack_blocks\", \"sweep_to_dustpan\", \n",
    "     \"pick_up_cup\",\n",
    "    \"toilet_seat_down\", \"close_fridge\", \n",
    "    \"take_usb_out_of_computer\", \n",
    "     \"water_plants\",\n",
    "    \"change_clock\", \n",
    "   #\n",
    "    \"take_lid_off_saucepan\"\n",
    "]\n",
    "root_dir =Path( \"/scratch/diffrac/datasets/vln-robot/\")\n",
    "dest_dir=Path(\"/home/pguhur/teaser_fig\")\n",
    "dest_dir.mkdir(exist_ok=True)\n",
    "variation = 0\n",
    "for task in tasks:\n",
    "    for episode in range(5):\n",
    "        front_rgb_dir = root_dir / f\"{task}/variation{variation}/episodes/episode{episode}/front_rgb/\"\n",
    "        if front_rgb_dir.is_dir():\n",
    "            break\n",
    "    assert front_rgb_dir.is_dir(),front_rgb_dir\n",
    "    front_rgbs = front_rgb_dir.glob(\"*.png\")\n",
    "    front_rgbs = natsorted(front_rgbs)\n",
    "    \n",
    "    front_mask_dir = root_dir / f\"{task}/variation{variation}/episodes/episode{episode}/front_mask/\"\n",
    "    front_masks = front_mask_dir.glob(\"*.png\")\n",
    "    front_masks = natsorted(front_masks)\n",
    "    \n",
    "    print(task, len(front_rgbs))\n",
    "    \n",
    "    for front_rgb in [front_rgbs[0]]:\n",
    "        # , front_rgbs[-1]]:\n",
    "        fid = int(front_rgb.stem)\n",
    "\n",
    "        img = Image.open(front_rgb)\n",
    "        timg = torch.from_numpy(np.asarray(img))\n",
    "\n",
    "        mask = Image.open(front_mask_dir/f\"{fid}.png\")\n",
    "        tmask = torch.from_numpy(np.asarray(mask))\n",
    "\n",
    "        objects = tmask.view(-1, 3).unique(dim=0)\n",
    "        # print(objects)\n",
    "        robot_mask = torch.zeros_like(tmask[:,:,0]).bool()\n",
    "        for i in range(31,47):\n",
    "            part = torch.Tensor([i, 0, 0]).view(1,1,3)\n",
    "            robot_mask |= (tmask == part).all(dim=2)\n",
    "\n",
    "        alpha_mask = Image.fromarray(robot_mask.int().numpy()*255).convert('L')\n",
    "        img.putalpha(alpha_mask)\n",
    "        img.save(dest_dir / f\"{task}-{variation}-{episode}-{fid}-mask.png\")\n",
    "        \n",
    "    img = Image.open(front_rgbs[-1])\n",
    "    img.save(dest_dir / f\"{task}-{variation}-{episode}-{fid}-img.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86d5ae-d74e-4660-9e9d-9b757392609e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Packaged sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869b3ed-b812-4e95-aead-e648a4033dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils import (\n",
    "    load_instructions\n",
    ")\n",
    "from create_data import RLBench\n",
    "\n",
    "# root_dir = \"/scratch/jeanzay/scratch/datasets/vln-robot/alhistory/datasets/real-dataset-0/\"\n",
    "# root_dir = \"datasets/real-dataset-0\"\n",
    "root_dir = \"datasets/\"\n",
    "\n",
    "taskvar = list(itertools.product(['tower3'], [0]))\n",
    "instruction = load_instructions(\"instructions-clip.pkl\")\n",
    "max_episode_length = 2\n",
    "dataset = RLBench(\n",
    "    root=root_dir,\n",
    "    taskvar=taskvar,\n",
    "    instructions=instruction,\n",
    "    gripper_pose='none',\n",
    "    taskvar_token=False,\n",
    "    max_episode_length=max_episode_length,\n",
    "    max_episodes_per_taskvar=100,\n",
    "    cache_size=100,\n",
    "    num_iters=100,\n",
    "    jitter=True,\n",
    "    training=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f060af1-5f87-4871-bc78-018373219a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "rgb = (sample['rgbs'] + 1) * 0.5\n",
    "rgb = rgb.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "for i in range(rgb.shape[0]):\n",
    "    plt.imshow(rgb[i, 0])\n",
    "    plt.title((\"{:.2f} \"*8).format(*sample['action'][i, :8].tolist()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7a1a0-b84e-4953-b99c-b886faf356c9",
   "metadata": {},
   "source": [
    "### Raw Sim2Real sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9950bc53-614f-4928-91ff-d5ea234bfa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_dir = Path(f'raw-dr/variation0/episodes/episode0/')\n",
    "demo = load_demo(ep_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361be357-3482-44e2-a59a-2ab9c48e34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "for i, f in enumerate(demo):\n",
    "    if f.left_shoulder_rgb is None:\n",
    "        continue\n",
    "        \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(getattr(f, 'left_shoulder_rgb'))\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(getattr(f, 'right_shoulder_rgb'))\n",
    "    \n",
    "    plt.title(f\"{i}: {f.gripper_open}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44a0b3-f8a8-4702-84f4-03e6ef104d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 563\n",
    "pcd = demo[idx].left_shoulder_pcd\n",
    "rgb = demo[idx].left_shoulder_rgb\n",
    "\n",
    "# pcd = pcd * (1 + 0.05*torch.empty_like(pcd).normal_())\n",
    "print(pcd.min(), pcd.max(), rgb.min(), rgb.max())\n",
    "\n",
    "mesh = o3d.geometry.PointCloud()\n",
    "mesh.points = o3d.utility.Vector3dVector(pcd.reshape(-1,3))\n",
    "mesh.colors = o3d.utility.Vector3dVector((rgb/255).reshape(-1,3))\n",
    "\n",
    "target_pos = demo[idx].gripper_pose[:3]\n",
    "radius = np.array([0.03]*3)\n",
    "target = o3d.geometry.AxisAlignedBoundingBox(target_pos - radius, target_pos + radius)\n",
    "\n",
    "draw([mesh, target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866649c2-64e6-449b-8dc3-a4c31af60809",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Debug sim2real dataset collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f345b6-ee6d-49ae-a550-84fb6af574d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collect_sim2real import collect_episode\n",
    "import gym\n",
    "env = gym.make('Var0-Stack-v0')\n",
    "episode = collect_episode(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2f0a5-2377-4854-88bf-e9324f68137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for frame in episode.buffer[-10:]:\n",
    "    plt.imshow(frame['rgb_charlie_camera'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a53ba7-8190-48c3-9980-d16a46a8f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "joint_vel = [\n",
    "    np.max(frame['arms_joint_vel']) for frame in episode.buffer\n",
    "]\n",
    "plt.semilogy(joint_vel, '+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f5db9-599a-485b-ac03-6fbaf0fb5019",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Predicting a position without PCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13272ca-53d6-4578-b7d2-232ca0beb847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predicted attn maps + samples\n",
    "obs = torch.load(\"obs.pth\")\n",
    "output = torch.load(\"output.pth\")\n",
    "rgb, pcd, gripper = get_rgb_pcd_gripper_from_obs(obs, \"none\")\n",
    "rgb = rgb.permute(0, 1, 3, 4, 2)\n",
    "rgb = (rgb + 1) * 0.5\n",
    "pcd = pcd.permute(0, 1, 3, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3ad31-c35c-402e-a4fe-cf9956d7afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, im in enumerate(rgb[0]):\n",
    "    plt.imshow(im.detach().cpu())\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7d030-1134-4f23-a115-7bff75b6c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, attn in enumerate(output['attention'][0]):\n",
    "    plt.imshow(attn[0].detach().cpu().log())\n",
    "    plt.title(i)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afdf9f-5b63-4afd-a4f5-7ee25c90c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fov = obs['info_charlie_camera']['fovy']\n",
    "attn = output['attention'][0, 2, 0]\n",
    "h, w = attn.shape\n",
    "\n",
    "xyz1 = obs['info_charlie_camera']['pos']\n",
    "euler1 = obs['info_charlie_camera']['euler']\n",
    "cam1_ext = get_extrinsics(xyz1, euler1)\n",
    "cam1_int = get_intrinsics(fov, h, w)\n",
    "\n",
    "xyz2 = np.array([0,0,0])\n",
    "euler2 = np.array([0,-math.pi/2,0])\n",
    "cam2_ext = get_extrinsics(xyz2, euler2)\n",
    "cam2_int = get_intrinsics(fov, h, w)\n",
    "\n",
    "# pixel coord of 4 corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366f505-abb2-4d82-8681-f3ab7be409ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pts_src = np.array(\n",
    "#   [\n",
    "#        [0, 0],\n",
    " #       [0, 127],\n",
    "  #      [127, 127],\n",
    "   #     [127, 0],\n",
    "   # ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd36006-1407-4fc0-ba8f-10d049de6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_src = np.array(\n",
    "    [\n",
    "        [80, 10],\n",
    "        [80, 100],\n",
    "        [25, 120],\n",
    "        [25, 40],\n",
    "    ]\n",
    ")\n",
    "\n",
    "word_coords = np.stack([\n",
    "    pcd[0, 1, i,j] for i, j in pts_src\n",
    "])\n",
    "print(word_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37765ad7-2ad4-4869-8de8-c1c8061839fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_coords = np.array(\n",
    "[[-0.65052414,  0.22975962,  0.00319054],\n",
    " [-0.6482557 , -0.14094314,  0.00319825],\n",
    " [-0.28 , -0.30831015,  0.12282395],\n",
    " [ -0.28,  0.20341617,  0.00665894]]\n",
    ")\n",
    "\n",
    "from utils_sim2real import world_coord_to_pixel\n",
    "fov = obs['info_charlie_camera']['fovy']\n",
    "attn = output['attention'][0, 2, 0]\n",
    "h, w = attn.shape\n",
    "xyz1 = obs['info_charlie_camera']['pos']\n",
    "euler1 = obs['info_charlie_camera']['euler']\n",
    "cam1_ext = get_extrinsics(xyz1, -euler1)\n",
    "#print(\"extrinsics\")\n",
    "#print(cam2_ext)\n",
    "cam1_int = get_intrinsics(fov, h, w)\n",
    "u,v,depth = world_coord_to_pixel(word_coords, cam1_ext, cam1_int, width=128)\n",
    "pts_src = np.stack([u, v], 1)\n",
    "print(u, v, depth)\n",
    "#pts_src[:, [1, 0]] = pts_src[:, [0, 1]]\n",
    "\n",
    "xyz2 = np.array([-0.6,0,0.7])\n",
    "euler2 = np.array([0,0,math.pi/2])\n",
    "cam2_ext = get_extrinsics(xyz2, -euler2)\n",
    "print(\"extrinsics\")\n",
    "print(cam2_ext)\n",
    "cam2_int = get_intrinsics(fov, h, w)\n",
    "\n",
    "pts_dst = world_coord_to_pixel(word_coords, cam2_ext, cam2_int, width=128)\n",
    "pts_dst = np.stack(pts_dst, 1)\n",
    "print(pts_dst)\n",
    "#pts_dst[:, [1, 0]] = pts_dst[:, [0, 1]]\n",
    "print(\"pts dest\")\n",
    "print(pts_dst)\n",
    "\n",
    "\n",
    "import cv2\n",
    "homography, status = cv2.findHomography(pts_src, pts_dst[:, :2])\n",
    "im_src = rgb[0][1].numpy()\n",
    "im_out = cv2.warpPerspective(im_src, homography, (im_src.shape[1],im_src.shape[0]))\n",
    "attn_log_out = cv2.warpPerspective(attn.log().detach().cpu().numpy(), homography, (im_src.shape[1],im_src.shape[0]))\n",
    "\n",
    "attn_out = cv2.warpPerspective(attn.detach().cpu().numpy(), homography, (im_src.shape[1],im_src.shape[0]))\n",
    "attn_out = torch.from_numpy(attn_out)\n",
    "\n",
    "plt.imshow(im_out)\n",
    "for pt in pts_src:\n",
    "    plt.scatter(pt[0], pt[1])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(attn_log_out)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d5ac7-fa9b-451a-96c1-b90357b88a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_out.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264c221-b716-414a-8c02-916d62db37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attn_out = torch.from_numpy(attn_out)\n",
    "from utils_sim2real import pixel_to_world_coord\n",
    "print(torch.max(attn_out))\n",
    "pos_ij = (attn_out==torch.max(attn_out)).nonzero()[0].tolist()\n",
    "pos_ijd = np.array([[*pos_ij, xyz2[2]]])\n",
    "print(pos_ijd)\n",
    "pixel_to_world_coord(pos_ijd, cam2_ext, cam2_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa03ae-ce5f-461f-920f-3612d5b388c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# attn mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d67749-ecee-4f36-a758-a14a1158d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Bernoulli\n",
    "p = 0.5\n",
    "dist = Bernoulli(torch.ones((B, T)) * p)\n",
    "diag = dist.sample()\n",
    "print(diag)\n",
    "torch.diag_embed(diag).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39c784-f337-417a-ae01-6fd2e3073492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "T = 6\n",
    "N = 3\n",
    "H = 8 \n",
    "W = 8\n",
    "B = 2\n",
    "a = torch.triu(torch.ones((T, T)), diagonal=1)\n",
    "attn = einops.repeat(a, 't tp -> b t k (tp kp)',  b=B, k=N, kp=N)\n",
    "\n",
    "dist = Bernoulli(torch.ones(B * T) * p)\n",
    "diag = dist.sample() .view(B, T)\n",
    "diag[:, 0] = 0\n",
    "mask = einops.repeat(torch.diag_embed(diag), 'b t tp -> b t k (tp kp)',  k=N, kp=N)\n",
    "\n",
    "# mask.long()\n",
    "print(diag)\n",
    "attn = (attn.bool() | mask.bool()).long()\n",
    "attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117e7ea-bf93-4973-bde8-f47fa593083a",
   "metadata": {},
   "source": [
    "## projecting the grippper position as an attn map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10251ec-f617-4cca-8adb-76d38a2f31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a demo\n",
    "\n",
    "env = RLBenchEnv(data_path=\"demos\", apply_rgb=True)\n",
    "task_str = 'stack_wine'\n",
    "task = task_file_to_task_class(task_str)\n",
    "demo = env.get_demo(task_str, 0, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff496ecc-6254-430b-8ada-4c5eefbd5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_ls = torch.load('state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74e9c7-77be-4944-aa1f-19deb5a90d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in state_ls:\n",
    "    for cam in step:    \n",
    "        plt.imshow(cam[0].permute(1,2,0))\n",
    "        plt.show()\n",
    "        plt.imshow(cam[1].permute(1,2,0))\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8531d4a-ceaa-40b5-9102-2fb673ec9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_ls2 = einops.rearrange(\n",
    "    state3,\n",
    "    \"(m cam ch) h w -> cam m h w ch\", ch=3, cam=3,m=2, \n",
    ")\n",
    "for cam in state_ls2:\n",
    "    # for cam in step:    \n",
    "        plt.imshow(cam[0])\n",
    "        plt.show()\n",
    "        plt.imshow(cam[1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87bc4a2-cbc4-41d9-a037-55b5784e83a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def obs_to_attn(obs) -> torch.Tensor:\n",
    "    extrinsics_44 = torch.from_numpy(obs.misc[f'{camera}_camera_extrinsics']).float()\n",
    "    extrinsics_44 = torch.linalg.inv(extrinsics_44)\n",
    "    intrinsics_33 = torch.from_numpy(obs.misc[f'{camera}_camera_intrinsics']).float()\n",
    "    intrinsics_34 = F.pad(intrinsics_33, (0, 1, 0, 0))\n",
    "    gripper_pos_3 = torch.from_numpy(obs.gripper_pose[:3]).float()\n",
    "    gripper_pos_41 = F.pad(gripper_pos_3, (0, 1), value=1).unsqueeze(1)\n",
    "    points_cam_41 = extrinsics_44 @ gripper_pos_41\n",
    "\n",
    "    proj_31 = intrinsics_34 @ points_cam_41\n",
    "    proj_3 = proj_31.float().squeeze(1)\n",
    "    u = (proj_3[0] / proj_3[2]).round().long()\n",
    "    v = (proj_3[1] / proj_3[2]).round().long()\n",
    "\n",
    "    attn = torch.zeros((128, 128))\n",
    "    if not (u < 0 or u > 127 or v < 0 or v > 127):\n",
    "        attn[v, u] = 1\n",
    "        \n",
    "    return attn, (u, v)\n",
    "\n",
    "for frame_id in keypoint_discovery(demo):\n",
    "    for camera in ('wrist', 'left_shoulder', 'right_shoulder'):\n",
    "        obs = demo[frame_id]\n",
    "        attn, (u, v) = obs_to_attn(obs)\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(getattr(obs, f'{camera}_rgb'))\n",
    "        plt.scatter(u, v, s=50, c='green', marker='o')\n",
    "        plt.plot([64, 64], [0, 128], c='blue')\n",
    "        plt.plot([0,128],[64, 64], c='blue')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(attn)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad1489-39ac-4280-af49-ad1079a6b14c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sim2Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844015c-1085-47e6-ac64-ee8e1aeb982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frames = []  \n",
    "#     with open(f'sim2real/raw/seed0/tower/variation0/episodes/0000_{frame:05}.pkl', 'rb') as fid:\n",
    "ep_id = 1\n",
    "# ep_dir = Path(f'raw-dr/variation0/episodes/')\n",
    "ep_dir = Path(f'/scratch/azimov/pguhur/datasets/tower/variation5/episodes/')\n",
    "files = list(sorted(ep_dir.glob(f'{ep_id:04}_*.pkl')))\n",
    "for f in tqdm(files):\n",
    "    with open(f, 'rb') as fid:\n",
    "        data = pickle.load(fid)\n",
    "    frames.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9280e2f-714a-4741-bbc0-2213a4bda035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgs = (Image.fromarray(f['rgb_charlie'][:, 280:-280]) for f in frames)\n",
    "img = next(imgs)  # extract first image from iterator\n",
    "img.save(fp=f'{ep_id}.gif', format='GIF', append_images=imgs,\n",
    "         save_all=True, duration=5, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954e77c-1686-46cd-8be4-159d14c45415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# world_coord2 = pixel_to_world_coord(np.array([pixel_coord]), extrinsics, intrinsics)\n",
    "world_coord = data[0]['cube1_pos'][None, :]\n",
    "print(world_coord)\n",
    "u, v, depth = world_coord_to_pixel(world_coord, extrinsics, intrinsics, width)\n",
    "print(u, v, depth)\n",
    "\n",
    "# pixel to pos2d\n",
    "pos_2d = np.array([[\n",
    "    (width - u)*depth,\n",
    "    v*depth,\n",
    "    depth\n",
    "]])\n",
    "print(transformed_coords_vectopostgres.confr)\n",
    "\n",
    "rgb = data[1]['rgb_charlie_camera'][:, 280:-280]\n",
    "plt.imshow(rgb)\n",
    "plt.scatter(u, v, s=100, c='y', marker='o')\n",
    "plt.scatter(u, v, s=20, c='r', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507168d-64c5-45fa-87b4-853af2cece37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation\n",
    "rgb = data[0]['rgb_charlie_camera0']# [:,280:-280]\n",
    "depth = data[0]['depth_charlie_camera0']#[:,280:-280]\n",
    "# real\n",
    "# rgb = data[1]['rgb_charlie_camera']#\n",
    "# depth = data[1]['depth_charlie_camera']#[:,280:-280]\n",
    "\n",
    "\n",
    "fov = float(data[0]['info_charlie_camera']['fovy'])\n",
    "euler = torch.Tensor(data[0]['info_charlie_camera']['euler'])\n",
    "pos = torch.Tensor(data[0]['info_charlie_camera']['pos'])\n",
    "extrinsics = get_extrinsics(pos, euler)\n",
    "\n",
    "depth2 = depth_resize(depth, extrinsics, fov, 128, 128)\n",
    "rgb = square(rgb)\n",
    "rgb2 = rgb_resize(rgb, 128, 128)/255\n",
    "\n",
    "h2, w2 = depth2.shape\n",
    "intrinsics2 = get_intrinsics(fov, h2, w2)\n",
    "pcd2 = depth_to_pcd(depth2, extrinsics, intrinsics2)\n",
    "mesh = o3d.geometry.PointCloud()\n",
    "mesh.points = o3d.utility.Vector3dVector(pcd2.reshape(h2*w2,3))\n",
    "mesh.colors = o3d.utility.Vector3dVector(rgb2.reshape(h2*w2, 3))\n",
    "points = mesh.get_axis_aligned_bounding_box().get_box_points()\n",
    "print('Array', np.asarray(points))\n",
    "bbox = o3d.geometry.AxisAlignedBoundingBox.create_from_points(points)\n",
    "\n",
    "# target_pos = data[0]['gripper_pos']\n",
    "target_pos = data[0]['cube0_pos']\n",
    "radius = np.array([0.03]*3)\n",
    "target = o3d.geometry.AxisAlignedBoundingBox(target_pos - radius, target_pos + radius)\n",
    "\n",
    "draw([mesh, bbox, target], show_ui=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689b23f-d641-49cd-a13e-b3d644d043bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import muse.envs\n",
    "from tqdm import trange\n",
    "\n",
    "seeds = set([])\n",
    "for var in trange(200):\n",
    "    env = gym.make(f\"Var{var}-PushButtons-v0\")\n",
    "    seeds |= set(env.seed(i) for i in range(2000))\n",
    "print(len(seeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c873a564-c710-457f-82e8-787c057d3a80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize samples from Sim2Real env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0498e-917f-4370-a4bf-ae3b70c65b7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d0838-fabf-4aca-bca2-db3422db66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/scratch/cooper/datasets/vln-robot/')\n",
    "env = RLBenchEnv(\n",
    "    data_path=data_path,\n",
    "    apply_rgb=True,\n",
    "    apply_pc=True,\n",
    "    apply_cameras=('front'),\n",
    ")\n",
    "\n",
    "task_str = 'tower3'\n",
    "task_type = task_file_to_task_class(task_str)\n",
    "# task = env.get_task(task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd5be6-b606-43a3-aa24-5b5638b23d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_inst = env.env.get_task(task_type)._task\n",
    "task_inst.init_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925e46b-6810-431b-b903-d25e086cf928",
   "metadata": {},
   "outputs": [],
   "source": [
    "instr = task_inst.init_episode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53654d7-5786-4985-972d-44cc37437cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a86bf-1cb1-4ab0-adcb-5ce5a430c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "demos = env.get_demo(task_str, 0, 0)\n",
    "demo = demos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6cc149-1249-4025-92f4-51afa86fbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = keypoint_discovery(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7e2bc-2c7c-4a74-b74e-f6c6d80349c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e5e30-26b9-4d7a-bd00-aa56030d8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[f].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899689c-d878-4996-92b5-a0578ff02a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo[f].right_shoulder_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac6405-aae3-4fe1-9a41-bfc03da2d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in frames:\n",
    "    img = Image.open(data_path / task_str / 'variation0' / 'episodes' / 'episode0' / 'front_rgb' / f'{f}.png')\n",
    "    plt.imshow(img)\n",
    "    plt.title(f)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94acb0a9-a5d5-4f44-8248-201e55a3e7a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# RLBench datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e52109-cdc6-47b4-a7ee-a64accc3ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "variation = 81\n",
    "data = np.load(f'/scratch/jeanzay/scratch/datasets/vln-robot/alhistory/dataset-{variation}/tower+0/ep1.npy', allow_pickle=True)\n",
    "\n",
    "plt.imshow(data[1][0][0,0].permute(1,2,0))\n",
    "# plt.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d1797-bd69-4e43-941e-b8369a7d5672",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Variations on RLBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc09c76-b13f-4d5f-86bc-7baddc77e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_mode = MoveArmThenGripper(\n",
    "    arm_action_mode=EndEffectorPoseViaPlanning(),\n",
    "    gripper_action_mode=Discrete(),\n",
    ")\n",
    "obs_config = ObservationConfig()\n",
    "env = Environment(\n",
    "    action_mode, 'datasets', obs_config, headless=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e08d75-6f8c-4142-b813-0102c3e583a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_str = 'push_buttons'\n",
    "task_type = task_file_to_task_class(task_str)\n",
    "task = env.get_task(task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94c537-ca4d-4954-84c1-5f8305cf9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(task._task.sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea29616-78c0-403a-8ed6-42ae50ed2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{task_str}.json', 'w') as fid:\n",
    "    json.dump(task._task.sequences[:300], fid, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085026f-cf54-4ee1-b5db-3fa855dbb678",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences= task._task.sequences[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aeb4d8-7e66-4018-b9e0-59f486ed3856",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('push_buttons.json') as fid:\n",
    "    sequences = json.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4f4ad-7946-45ef-a21d-269af529ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_colors = set(\n",
    "    [\"yellow\", \"green\", \"cyan\",\n",
    "     \"blue\", \"white\", \"purple\",\n",
    "     \"black\", \"orange\", \"rose\", \"maroon\"\n",
    "])\n",
    "counter = 0\n",
    "#for i, ((new_step, orig_step), s) in enumerate(sequences[:200]):\n",
    "for i, s in enumerate(sequences[:200]):\n",
    "    objects = tuple(sorted(dict(s).keys()))\n",
    "    if all(o in our_colors for o in objects):\n",
    "        print(i, s)\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0fed1-fefe-4d7c-b590-a2102cc031b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "objects_to_id ={}\n",
    "same_objects = []\n",
    "for i, s in enumerate(task._task.sequences[:200]):\n",
    "    # print(i, s)\n",
    "    objects = tuple(sorted(dict(s).keys()))\n",
    "    # if  \"cyan\" in objects and not \"navy\" in objects:\n",
    "    #     print(i, objects)\n",
    "        # == ('blue', 'navy', 'cyan'):\n",
    "    # if len(objects) == 1:\n",
    "    # if 'green' in objects and 'teal' in objects:\n",
    "        # print(i, s)\n",
    "    if objects in objects_to_id:\n",
    "        print(i, objects_to_id[objects], objects)\n",
    "        same_objects.append((i, objects_to_id[objects]))\n",
    "    else:\n",
    "        objects_to_id[objects] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef7220-ccde-43d0-adfb-6019fe5579b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('tower.json') as fid:\n",
    "    sequences = json.load(fid)\n",
    "\n",
    "our_colors = set(\n",
    "    [\"yellow\", \"green\", \"cyan\",\n",
    "     \"blue\", \"white\", \n",
    "     \"black\", \"orange\", \"rose\", \"red\", \"lime\"\n",
    "])\n",
    "counter = 0\n",
    "for i, s in enumerate(sequences[:200]):\n",
    "    objects = tuple(sorted(dict(s).keys()))\n",
    "    if all(o in our_colors for o in objects):\n",
    "        print(i, s)\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b19d7-74b9-431e-abd1-caeb19f13089",
   "metadata": {},
   "outputs": [],
   "source": [
    "variations = [('B', 'P', 'G')]\n",
    "new_variations = variations.copy()\n",
    "for c in variations[0]:\n",
    "    print(c, flush=True)\n",
    "    new_variations += list(itertools.permutations(list(variations[0]) + [c]))\n",
    "    print(len(new_variations), len(variations), flush=True)\n",
    "print(new_variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c59824-4bcf-4ba9-b4db-ce98954f5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(itertools.permutations(variations + [c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6eebbd-9cb6-4087-bceb-d44ce18aa0d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# C2FARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142c9e3-b791-4e9f-ad99-dcbae94ce2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "root_dir = Path('/scratch/jeanzay/scratch/logs/vln-robot/c2farm/10/seed0/')\n",
    "data_csv = sorted(list(root_dir.glob('*/C2FARM+QTE/seed*/data.csv')))\n",
    "for data in data_csv:\n",
    "    task = data.parents[2].name\n",
    "    method = data.parents[1].name\n",
    "    res = pd.read_csv(data)\n",
    "    ret = res['train_envs/return']\n",
    "    n = ret.shape[0]\n",
    "    print(f\"{task}-{method}\", '\\ttrain max', ret.max(), '\\ttrain mean last 10%', ret[int(n*0.9):].mean())\n",
    "    # plt.title(f\"{task}-{method}\")\n",
    "    # plt.plot(data['eval_envs/return'])\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27888b14-8a48-4ced-8183-692653d8251c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc4440-8d01-4db5-89a5-ecbef8cfd101",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = ['taskvar', 'ckpt', 'seed', 'sr', 'test']\n",
    "df = pd.read_csv('/home/pguhur/Desktop/records-jz.csv', names=columns)\n",
    "df.drop(columns=['seed', 'test'], inplace=True)\n",
    "\n",
    "# discard multitask / multivar\n",
    "multi = df['ckpt'].str.contains('\\+100|\\+45|multi')\n",
    "df = df[~multi]\n",
    "\n",
    "# discard obsolete xp\n",
    "# zinstr = df['ckpt'].str.contains('zinstr')\n",
    "# df = df[zinstr]\n",
    "\n",
    "# keep only 10-tasks\n",
    "with open('10-tasks.txt') as fid:\n",
    "    tasks = [f\"{t.strip()}-0\"  for t in fid.readlines()]\n",
    "tasks_search  = \"|\".join(tasks)\n",
    "print(tasks, tasks_search)\n",
    "df = df[df['taskvar'].str.contains(tasks_search)]\n",
    "# for t in tasks:\n",
    "#     df[t] = df['taskvar'].str.contains(f'{t}-')\n",
    "\n",
    "df['Visual Tokens'] = np.select([\n",
    "        df['ckpt'].str.contains('plain'),\n",
    "        df['ckpt'].str.contains('tnc'),\n",
    "        df['ckpt'].str.contains('tnhw'),\n",
    "    ], ['X', 'Channel', 'Patch'])\n",
    "df['Point Clouds'] = np.where(df['ckpt'].str.contains('pcd'), 'V', 'X')\n",
    "df['Gripper Position'] = np.where(df['ckpt'].str.contains('gp_attn|gripper'), 'V', 'X')\n",
    "\n",
    "df['Multiview'] = np.where(df['ckpt'].str.contains('plain'), 'X', 'V')\n",
    "df['History'] = np.where(df['ckpt'].str.contains('stateless|plain'), 'X', 'V')\n",
    "df['Attn'] = np.select([\n",
    "        df['ckpt'].str.contains('plain'),\n",
    "        df['ckpt'].str.contains('_cm'),\n",
    "        ~df['ckpt'].str.contains('_cm'),\n",
    "    ], ['X', 'Cross', 'Self'])\n",
    "\n",
    "df['Mask Obs'] = np.where(df['ckpt'].str.contains('mask'), 'V', 'X')\n",
    "\n",
    "# Table\n",
    "# \tVisual Tokens\tPoint Clouds\tGripper Position\tMultiview\tHistory\tAttn\tMask Obs\n",
    "# R1\tX\tX\tX\tX\tX\tX\tX\n",
    "# R2\tChannel\tX\tX\tV\tX\tSelf\tX\n",
    "# R3\tChannel\tV\tX\tV\tX\tSelf\tX\n",
    "# R4\tChannel\tV\tV\tV\tX\tSelf\tX\n",
    "# R5\tChannel\tV\tV\tV\tV\tSelf\tX\n",
    "# R6\tChannel\tV\tV\tV\tV\tSelf\tV\n",
    "# R7\tPatch\tV\tV\tV\tV\tSelf\tV\n",
    "# R8\tPatch\tV\tV\tV\tV\tCross\tV\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3923c4-6bd2-40b0-a2ad-1ba095ce8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = ['Visual Tokens', 'Point Clouds', 'Gripper Position', 'Multiview', 'History', 'Attn', 'Mask Obs']\n",
    "agg = df.groupby(by=['taskvar'] + criteria).agg({\"sr\": [np.mean, np.std, np.size]})\n",
    "table = agg.groupby(by=criteria).agg({('sr', 'mean'): [np.mean], ('sr', 'std'): np.mean, ('sr', 'size'): np.sum})\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c26d5d-2813-4e9e-b054-54cfcf11bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation = pd.read_csv('ablation.csv',index_col=0)\n",
    "ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56413c16-d584-479c-8887-0d4aa9d3f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9553b-f155-4d23-a507-e65f470499b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_id, row in ablation.iterrows():\n",
    "    # masks = [df[key] == value for key, value in row.items()]\n",
    "    # mask = masks[0]\n",
    "    # for m in masks[1:]:\n",
    "    #     mask &= m\n",
    "    index = int(row_id[1:])\n",
    "    mask = df['ckpt'].str.contains(f'row{index}')\n",
    "    \n",
    "    if index > 5:\n",
    "        mask &= (\n",
    "            df['ckpt'].str.contains(f'mask2') |\n",
    "            df['taskvar'].str.contains(\n",
    "                f'pick_up_cup|push_button|reach_target|slide_block_to_target|take_umbrella'\n",
    "            ) \n",
    "        )\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(row_id, row)\n",
    "    \n",
    "    print('-'*80, '\\n', 'Items')\n",
    "    df_mask = df[mask]\n",
    "    print(df_mask)\n",
    "    \n",
    "    print('-'*80, '\\n', 'Agg/tasks')\n",
    "    agg = df_mask.groupby(by=['taskvar'] + criteria).agg({\"sr\": [np.mean, np.std, np.size]})\n",
    "    print(agg)\n",
    "    \n",
    "    print('-'*80, '\\n', 'Agg/row')\n",
    "    table = agg.groupby(by=criteria).agg({('sr', 'mean'): [np.mean], ('sr', 'std'): np.mean, ('sr', 'size'): np.sum})\n",
    "    print(table)\n",
    "\n",
    "    print(\"\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2b102-5696-45bd-baf5-aac6a93d2f47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Versatility Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53dd67-86e5-4fe7-9454-4460391409e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sv = pd.read_csv('/home/pguhur/Desktop/records-singleview.csv', names=columns)\n",
    "df_sv.drop(columns=['seed', 'test'], inplace=True)\n",
    "mask = df_sv[\"ckpt\"].str.contains(\"singleview\")\n",
    "df_sv = df_sv[mask]\n",
    "df_sv['cam'] = df_sv['ckpt'].apply(lambda x: x.split('singleview-')[1].split('_')[0])\n",
    "df_sv = df_sv.groupby(by=['taskvar']).agg({\"sr\": np.max})\n",
    "df_sv = df_sv.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c6909-a8c2-438b-b8f2-1c83c5d7b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sv.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219a86a-cab5-41b9-b15f-e8c6d07975d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d83566-34e1-45f9-85d4-dcab851607a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = ['taskvar', 'ckpt', 'seed', 'sr', 'test']\n",
    "df_mv = pd.read_csv('/home/pguhur/Desktop/records2.csv', names=columns, header=1)\n",
    "df_mv.drop(columns=['seed', 'test'], inplace=True)\n",
    "\n",
    "df_st = pd.read_csv('/home/pguhur/Desktop/records-jz.csv', names=columns)\n",
    "df_st.drop(columns=['seed', 'test'], inplace=True)\n",
    "\n",
    "df_sv = pd.read_csv('/home/pguhur/Desktop/records-singleview.csv', names=columns)\n",
    "df_sv.drop(columns=['seed', 'test'], inplace=True)\n",
    "mask = df_sv[\"ckpt\"].str.contains(\"singleview\")\n",
    "df_sv = df_sv[mask]\n",
    "df_sv['cam'] = df_sv['ckpt'].apply(lambda x: x.split('singleview-')[1].split('_')[0])\n",
    "df_sv = df_sv.groupby(by=['taskvar']).agg({\"sr\": np.max})\n",
    "df_sv = df_sv.reset_index()\n",
    "\n",
    "df_al = pd.read_csv('/home/pguhur/Desktop/records-al.csv', names=['task', 'sr', \"etc\"])\n",
    "mask = df_al[\"sr\"].str.contains(\"seed\")\n",
    "df_al.loc[mask, \"sr\"]  = pd.to_numeric(df_al.loc[mask, \"etc\"])\n",
    "\n",
    "task_desc = pd.read_csv(\"versatility.csv\")\n",
    "# task_desc.set_index(\"Task\")\n",
    "\n",
    "def found_result(task_name, df):\n",
    "    sr = df[df['taskvar'].str.contains(task_name) & df['ckpt'].str.contains(\"tnhw_cm.*mask2\")][\"sr\"]\n",
    "    if sr.size == 0:\n",
    "        sr = df[df['taskvar'].str.contains(task_name) & df['ckpt'].str.contains(\"tnhw_cm.*mask\")][\"sr\"]\n",
    "    if sr.size == 0:\n",
    "        sr = df[df['taskvar'].str.contains(task_name) & df['ckpt'].str.contains(\"tnhw_cm.*\")][\"sr\"]\n",
    "    if sr.size == 0:\n",
    "        sr = df[df['taskvar'].str.contains(task_name)][\"sr\"]  \n",
    "    return sr.mean()\n",
    "\n",
    "# add model results.\n",
    "task_desc['MV'] = task_desc.apply(\n",
    "    lambda row : found_result(row[\"Task\"], df_mv), axis = 1\n",
    ")\n",
    "\n",
    "def found_result(task_name, df):\n",
    "    sr = df[df['taskvar'].str.contains(task_name) & df['ckpt'].str.contains(\"tnhw_cm.*mask2\")][\"sr\"]\n",
    "    if sr.size == 0:\n",
    "        sr = df[df['taskvar'].str.contains(task_name) & df['ckpt'].str.contains(\"tnhw_cm.*mask\")][\"sr\"]\n",
    "    if sr.size == 0:\n",
    "        sr = df[df['taskvar'].str.contains(task_name) & df['ckpt'].str.contains(\"tnhw_cm.*\")][\"sr\"]\n",
    "    if sr.size == 0:\n",
    "        sr = df[df['taskvar'].str.contains(task_name)][\"sr\"]  \n",
    "    return sr.mean()\n",
    "\n",
    "task_desc['Stateless'] = task_desc.apply(\n",
    "    lambda row : found_result(row[\"Task\"], df_st), axis = 1\n",
    ")\n",
    "\n",
    "def found_result3(task_name, df):\n",
    "    sr = df[df['taskvar'].str.contains(task_name)][\"sr\"]  \n",
    "    return sr.mean()\n",
    "\n",
    "task_desc['SV'] = task_desc.apply(\n",
    "    lambda row : found_result3(row[\"Task\"], df_sv), axis = 1\n",
    ")\n",
    "\n",
    "def found_result2(task_name, df):\n",
    "    sr = df[df['task'].str.contains(task_name)][\"sr\"]\n",
    "    return pd.to_numeric(sr).mean()\n",
    "\n",
    "task_desc['AL'] = task_desc.apply(\n",
    "    lambda row : found_result2(row[\"Task\"], df_al), axis = 1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "nan = task_desc.isnull().values.any(1)\n",
    "print(task_desc[nan])\n",
    "print('# Nan', nan.sum())\n",
    "print('# Task', (~nan).sum())\n",
    "task_desc = task_desc[~nan]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff322d0-a1ba-4c74-8b68-a2eab7b88e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by = task_desc.groupby(by=['Set']).agg({\n",
    "    \"MV\": [np.mean, np.size], \n",
    "    \"AL\": [np.mean, np.size], \n",
    "    \"Stateless\": [np.mean, np.size],\n",
    "    \"SV\": [np.mean, np.size]\n",
    "})\n",
    "group_by = group_by.sort_index()\n",
    "group_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca85cc7-6b7b-4a13-abbe-78258a4223d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = task_desc[task_desc[\"MV\"] > 0.44][\"Task\"]\n",
    "tasks.sample(n=50, replace=False).to_csv(\"50-tasks.csv\", header=False, index=False)\n",
    "tasks.sample(n=30, replace=False).to_csv(\"30-tasks.csv\", header=False, index=False)\n",
    "task_desc.sample(n=70, replace=False)[\"Task\"].to_csv(\"70-tasks.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce648439-8fc3-4e95-bd01-03574f170ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "methods = ['MV', 'AL']\n",
    "for i, method in enumerate(methods):\n",
    "    series = sorted(list(task_desc.groupby(by=['Set'])), key=lambda s: s[1]['MV'].mean(), reverse=False)\n",
    "    legends = [s[0] for s in series]\n",
    "    x = [np.asarray(s[1][method]) for s in series]\n",
    "    ax = plt.subplot(1,2,i+1)\n",
    "    hist = plt.hist(x,\n",
    "                    histtype='stepfilled',\n",
    "                    cumulative=-1,\n",
    "              bins=100,  stacked=True)\n",
    "    if i == len(methods) - 1:\n",
    "        plt.legend(legends, loc='upper right')\n",
    "    plt.xlabel('Threshold as % of success rate')\n",
    "    plt.ylabel('Total number of tasks above this threshold')\n",
    "    plt.xlim([0,1])\n",
    "\n",
    "# handles, labels = ax.get_legend_handles_labels()\n",
    "# fig.legend(handles, labels, loc='upper center')\n",
    "plt.savefig(f'cum-sr.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cfb0d5-330d-41b0-9c1f-0ab822357ef3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Convert tower annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd116e-b1d9-4f69-935b-8a770a329cc2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/home/pguhur/Downloads/amt-train.fixed.json\") as fid:\n",
    "    data = json.load(fid)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e164095-b944-44c9-b6f3-10cf56cf3703",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = data[0]\n",
    "colors = [item[\"color\"][1]] + item[\"color\"][:-1:2]\n",
    "variations = \n",
    "print(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fee7e8-da93-4e1c-adca-548e6c12fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [ {\n",
    "    \"id\": i,\n",
    "    \"createdTime\": \"2020-01-01T00:00:00\",\n",
    "    \"fields\": {\n",
    "    \"instruction\": item['sentence'],\n",
    "    \"task\": \"tower3\",\n",
    "    \"variation\": get_variation(item)\n",
    "    }\n",
    "} for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32293a1-dc97-4a1e-b443-9f2a77c54232",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Checking on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc78ba-45d7-47d0-b1d8-64067dddc5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('instructions-clip.pkl', 'rb') as fid:\n",
    "    embeddings = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59f754-7fc5-4e91-b4e0-6204a8727ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = [65, 53]\n",
    "for v in var:\n",
    "    print(task._task.sequences[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4022a69-9fca-488c-936e-64c304b6eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbs = [embeddings['push_buttons'][v] for v in var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9651f-a9a9-40b9-9650-65a10956fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [p[0].mean(0) for p in pbs]\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.imshow(means[0].view(16,32))\n",
    "plt.title('Combaring avg embeddings for 2 variations w/ the same objects')\n",
    "plt.colorbar()\n",
    "plt.subplot(312)\n",
    "plt.imshow(means[1].view(16,32))\n",
    "plt.colorbar()\n",
    "plt.subplot(313)\n",
    "plt.imshow((means[1]-means[0]).view(16,32))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531eaf9-9ace-4ca6-94cf-f33eb8f70738",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [p[0].mean(0) for p in pbs]\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.subplot(311)\n",
    "plt.imshow(means[0].view(16,32))\n",
    "plt.title('Combaring avg embeddings for 2 tasks')\n",
    "plt.colorbar()\n",
    "plt.subplot(312)\n",
    "e2 = embeddings['tower3'][0][0].mean(0).view(16,32)\n",
    "plt.imshow(e2)\n",
    "plt.colorbar()\n",
    "plt.subplot(313)\n",
    "plt.imshow(means[1].view(16,32) - e2)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86734a-0b20-443a-8552-0987b0d1ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [p[0].mean(0) for p in pbs]\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.subplot(311)\n",
    "plt.imshow(means[0].view(16,32))\n",
    "plt.title('Combaring avg embeddings for 2 variations with different objects')\n",
    "plt.colorbar()\n",
    "plt.subplot(312)\n",
    "e2 = embeddings['push_buttons'][0][0].mean(0).view(16,32)\n",
    "plt.imshow(e2)\n",
    "plt.colorbar()\n",
    "plt.subplot(313)\n",
    "plt.imshow(means[1].view(16,32) - e2)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97164c75-6f7d-440c-a594-cb4d08af2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(embeddings['tower3'][0][0].mean(0).view(16,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a400f3-b53c-43c0-8f76-be4d78fe0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(embeddings['push_buttons'][0][0,30].view(16,32))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5de182-0c17-4575-b691-9eaaca42aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['push_buttons'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377139ac-ffd8-48cd-843e-45333ea186d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['push_buttons'][65][0].mean(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582cbfb3-8a36-40c2-86fc-cf52d2db2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cosine_similarity(\n",
    "             embeddings['push_buttons'][65][0].mean(0),\n",
    "             embeddings['push_buttons'][53][0].mean(0),\n",
    "            dim=0\n",
    "        ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7bdd6-f5b8-451e-913b-532327416614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import itertools\n",
    "print('same task, same objects')\n",
    "cossims = []\n",
    "for var1, var2 in same_objects:\n",
    "    # if var1 in embeddings['push_buttons'] and var2 in embeddings['push_buttons']\n",
    "    cossim = F.cosine_similarity(\n",
    "             embeddings['push_buttons'][var1][:2].mean(1),\n",
    "             embeddings['push_buttons'][var2][:2].mean(1),\n",
    "            dim=1\n",
    "        ).max()\n",
    "        # \n",
    "    cossims.append(cossim)\n",
    "print('\\t',torch.Tensor(cossims).mean())\n",
    "\n",
    "\n",
    "print('same task, different objects')\n",
    "cossims = []\n",
    "for var1, var2 in itertools.product(range(100), range(100)):\n",
    "    if (var1, var2) in same_objects or var1 == var2:\n",
    "        continue\n",
    "    cossim = F.cosine_similarity(\n",
    "             embeddings['push_buttons'][var1][:2].mean(1),\n",
    "             embeddings['push_buttons'][var2][:2].mean(1),\n",
    "            dim=1\n",
    "        ).max()\n",
    "        # \n",
    "    cossims.append(cossim)\n",
    "print('\\t',torch.Tensor(cossims).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc8fc3-dfa4-4837-a370-0e57d5f4ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('different task')\n",
    "cossims = []\n",
    "for task1, task2 in itertools.product(embeddings.keys(), embeddings.keys()):\n",
    "    if task1 == task2:\n",
    "        continue\n",
    "    cossim = F.cosine_similarity(\n",
    "             embeddings[task1][0][:2].mean(1),\n",
    "             embeddings[task2][0][:2].mean(1),\n",
    "            dim=1\n",
    "        ).max()\n",
    "        # \n",
    "    cossims.append(cossim)\n",
    "print('\\t',torch.Tensor(cossims).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88580e-71d1-43cb-b801-fcfec3a1fbca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Visualize prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9721c3c-4efd-4287-9f8d-8e513db919d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_task_baselines import get_model, collate_fn, Arguments as TrainArguments\n",
    "\n",
    "dataset = \"/scratch/jeanzay/scratch/datasets/real-dataset-0/\"\n",
    "sys.argv = [\n",
    "    'foo', '--dataset', str(dataset), \n",
    "    '--tasks', 'push_buttons'\n",
    "]\n",
    "\n",
    "args = TrainArguments().parse_args()\n",
    "args.checkpoint = \"xp-jz/hf-s2r+noise_push_buttons/version521949/model.step=90-value=0.pth\"\n",
    "args.tr_token = \"tnhw_cm\"\n",
    "args.pcd_token = 'none' \n",
    "args.gripper_pose = \"none\"\n",
    "args.z_mode = \"instr2\"\n",
    "args.temp_len = 0\n",
    "args.instructions = \"instructions-clip.pkl\"\n",
    "optimizer, meta_model, loss_and_metrics = get_model(args)\n",
    "model = meta_model[\"model\"]\n",
    "t_dict = meta_model[\"t\"]\n",
    "z_dict = meta_model[\"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55118c-c137-41d8-b4be-8720a71c9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path(\"/scratch/cooper/datasets/hiveformer/s2r-dr-dataset-0/\")\n",
    "# dataset = Path(\"/scratch/cooper/datasets/hiveformer/real-dataset-0/\")\n",
    "taskvar = [('push_buttons', 0)]\n",
    "instruction = load_instructions(\"instructions-clip.pkl\")\n",
    "dataset = RLBench(\n",
    "    root=dataset,\n",
    "    taskvar=taskvar,\n",
    "    instructions=instruction,\n",
    "    gripper_pose=args.gripper_pose,\n",
    "    taskvar_token=False,\n",
    "    max_episode_length=6,\n",
    "    max_episodes_per_taskvar=100,\n",
    "    cache_size=0,\n",
    "    jitter=True,\n",
    "    training=True,\n",
    "    cameras=('wrist', 'left_shoulder', 'right_shoulder'),\n",
    ")\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653381fe-1a01-4f74-8f9b-6e4fca1768cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_loader = iter(loader)\n",
    "sample = next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a170a80-b022-4693-8c97-743fc2ec2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf72047-9f9f-42ae-a46a-ab06dd69a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t_dict[list(t_dict.keys())[0]].device\n",
    "\n",
    "rgbs = sample[\"rgbs\"].to(device)\n",
    "pcds = sample[\"pcds\"].to(device)\n",
    "gripper = sample[\"gripper\"].to(device)\n",
    "outputs = sample[\"action\"].to(device)\n",
    "padding_mask = sample[\"padding_mask\"].to(device)\n",
    "\n",
    "instr = sample[\"instr\"]\n",
    "if instr is not None:\n",
    "    instr = instr.to(device)\n",
    "\n",
    "taskvar = sample[\"taskvar\"]\n",
    "if taskvar is not None:\n",
    "    taskvar = taskvar.to(device)\n",
    "\n",
    "frame_id = sample[\"frame_id\"]\n",
    "tasks = sample[\"task\"]\n",
    "t = torch.stack([t_dict[task][fid] for task, fid in zip(tasks, frame_id)])\n",
    "z = torch.stack([z_dict[task][fid] for task, fid in zip(tasks, frame_id)])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(\n",
    "        rgbs,\n",
    "        pcds,\n",
    "        padding_mask,\n",
    "        t,\n",
    "        z,\n",
    "        instr,\n",
    "        gripper,\n",
    "        taskvar,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb629b9c-4c4a-488f-afbd-24213d8d7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbs = (sample[\"rgbs\"][0].cpu() + 1) * 0.5\n",
    "rgbs = rgbs.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "for step_id, (attns, rgb) in enumerate(zip(pred['attention'], rgbs)):\n",
    "    for cam_id, (a, r) in enumerate(zip(attns[1:], rgb[1:3])):\n",
    "        i, j = (a[0] == a[0].max()).nonzero()[0].cpu()\n",
    "        \n",
    "        plt.subplot(2, 2, cam_id + 1)\n",
    "        plt.imshow(r)\n",
    "        plt.scatter(j,i, color=\"r\", marker=\"+\")\n",
    "        \n",
    "        plt.subplot(2, 2, cam_id + 3)\n",
    "        plt.imshow(a[0].detach().cpu().log())\n",
    "        plt.scatter(j,i, color=\"r\", marker=\"+\")\n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2415f1-3487-4420-81c3-b9f72061d523",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Increase difficulty of the task PushButtons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e7abaa-dea0-40e0-8ccb-5d669c5ff269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils import (\n",
    "    load_instructions\n",
    ")\n",
    "from create_data import RLBench\n",
    "\n",
    "taskvar = list(itertools.product(['push_buttons'], [0]))\n",
    "instruction = load_instructions(\"instructions-clip.pkl\")\n",
    "max_episode_length = 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563b22d-abb8-4f63-a571-8e43ed84d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_ids = sample[0].copy()\n",
    "state_ls = sample[1].copy()\n",
    "action_ls = sample[2].copy()\n",
    "attn_indices = sample[3].copy()\n",
    "gripper_pos = sample[4].copy()\n",
    "\n",
    "orig_step, new_step = new_variations[0][0]\n",
    "frame_ids += [len(frame_ids), len(frame_ids) + 1]\n",
    "state_ls.insert(1 + 2 * new_step, sample[1][1 + 2 * orig_step])\n",
    "state_ls.insert(2 + 2 * new_step, sample[1][2 + 2 * orig_step])\n",
    "action_ls.insert(1 + 2 * new_step, sample[2][1 + 2 * orig_step])\n",
    "action_ls.insert(2 + 2 * new_step, sample[2][2 + 2 * orig_step])\n",
    "attn_indices.insert(1 + 2 * new_step, sample[3][1 + 2 * orig_step])\n",
    "attn_indices.insert(2 + 2 * new_step, sample[3][2 + 2 * orig_step])\n",
    "gripper_pos.insert(1 + 2 * new_step, sample[4][1 + 2 * orig_step])\n",
    "gripper_pos.insert(2 + 2 * new_step, sample[4][2 + 2 * orig_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da19f5-a24b-40d8-852d-338f1fb3ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.stack(state_ls)\n",
    "rgbs = states[:, :, 0]\n",
    "rgbs = (rgbs + 1) *0.5\n",
    "rgbs = rgbs.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "for i in range(rgbs.shape[0]):\n",
    "    plt.imshow(rgbs[i, 0])\n",
    "    # plt.title((\"{:.2f} \"*8).format(*sample['action'][i, :8].tolist()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f5e704-053e-4e06-810b-0d22e3fd8c3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Wav2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fccb6-162b-439a-a2eb-a0533d2ea177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "def record_mic(duration: float, fs: int) -> np.ndarray:\n",
    "    return sd.rec(duration * fs, samplerate=fs, channels=1, dtype='float64')\n",
    "\n",
    "class MicToText:\n",
    "    def __init__(self, duration: float = 5., sampling_rate: int = 44100, device: str = 'cpu'):\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.model = self.model.to(device).eval()\n",
    "        self.duration = duration\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    def __call__(self) -> str:\n",
    "        record = record_mic(self.duration, self.sampling_rate)\n",
    "        \n",
    "        # audio file is decoded on the fly\n",
    "        inputs = self.processor(record, sampling_rate=self.sampling_rate, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # transcribe speech\n",
    "        transcription = processor.batch_decode(predicted_ids)\n",
    "        return transcription[0]\n",
    "    \n",
    "class TextToToken:\n",
    "    def __init__(self, model_max_length: int = 42, device: str = 'cpu'):\n",
    "        self.model = transformers.CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.model = self.model.to(device)\n",
    "        self.device = device\n",
    "        self.model_max_length = model_max_length\n",
    "        self.tokenizer = transformers.CLIPTokenizer.from_pretrained(\n",
    "            \"openai/clip-vit-base-patch32\"\n",
    "        )\n",
    "        self.tokenizer.model_max_length = model_max_length\n",
    "    \n",
    "    def __call__(self, instruction: str) -> torch.Tensor:\n",
    "        tokens = tokenizer(instruction, padding=\"max_length\")[\"input_ids\"]\n",
    "        lengths = [len(t) for t in tokens]\n",
    "        if any(l > self.model_max_length for l in lengths):\n",
    "            raise RuntimeError(f\"Too long instructions: {lengths}\")\n",
    "\n",
    "        tokens = torch.tensor(tokens).to(self.device)\n",
    "        with torch.no_grad():\n",
    "                pred = model(tokens).last_hidden_state\n",
    "            \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3885cc-7955-473b-b74e-4d396e0fb2ce",
   "metadata": {},
   "source": [
    "# Instructions Push Repeated Buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab434be-f780-4dea-a7a7-dbb76446e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('push_repeated_buttons.json') as fid:\n",
    "    variations = json.load(fid)\n",
    "variations[124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69365840-3fb4-4caa-8c1e-39687ffe6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_push_buttons import generate_repeated_buttons_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d6c38-8e12-4b16-b875-a988ed68353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_repeated_buttons_instructions(['rose', 'orange', 'black'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05caf438-a46b-4e2b-92a1-08b0621b88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotations2.json') as fid:\n",
    "    data = json.load(fid)\n",
    "    \n",
    "with open('push_repeated_buttons.json') as fid:\n",
    "    variations = json.load(fid)\n",
    "variations = {\n",
    "    i: [c for c, rgb in s] for i, (_, s) in enumerate(variations)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54a050-0c2b-4856-a896-c4cccd6c62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = []\n",
    "\n",
    "for item in data:\n",
    "    var = item[\"fields\"][\"variation\"]\n",
    "    colors = variations[var]\n",
    "    if len(colors) != len(set(colors)):\n",
    "        item[\"fields\"][\"instr_old\"] = item[\"fields\"][\"instruction\"]\n",
    "        item[\"fields\"][\"instruction\"] = \"\"\n",
    "    data2.append(item)\n",
    "    \n",
    "with open('annotations2.json', \"w\") as fid:\n",
    "    json.dump(data,fid,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22e39b-7fa3-440d-9b00-a88c5971f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotations2.json') as fid:\n",
    "    data = json.load(fid)\n",
    "    \n",
    "for item in data:\n",
    "    if item[\"fields\"][\"instruction\"] == \"\":\n",
    "        var = item[\"fields\"][\"variation\"]\n",
    "        colors = variations[var]\n",
    "        print(var, colors)\n",
    "        print(item[\"fields\"][\"instr_old\"])\n",
    "        item[\"fields\"][\"instruction\"] = input(\">\")\n",
    "        \n",
    "        with open('annotations2.json', \"w\") as fid:\n",
    "            json.dump(data,fid, indent=2)\n",
    "            \n",
    "with open('annotations2.json', \"w\") as fid:\n",
    "            json.dump(data,fid, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107092f-b0ed-4903-88db-bd18930675fc",
   "metadata": {},
   "source": [
    "# Silva et al"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef03e28-343b-4f1f-a609-ee178b10ca70",
   "metadata": {},
   "source": [
    "### Obtaining pose & velocity of each object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1153ac-c927-495c-89af-7f4fa1d9464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from os.path import join, dirname, abspath, isfile\n",
    "\n",
    "# sys.path.insert(0, join(CURRENT_DIR, '..'))  # Use local RLBench rather than installed\n",
    "\n",
    "import traceback\n",
    "import readline\n",
    "\n",
    "from pyrep.const import RenderMode\n",
    "\n",
    "from rlbench.backend import task\n",
    "from rlbench.backend.const import TTT_FILE\n",
    "from pyrep import PyRep\n",
    "from pyrep.robots.arms.panda import Panda\n",
    "from pyrep.objects.shape import Shape\n",
    "from pyrep.objects.proximity_sensor import ProximitySensor\n",
    "from pyrep.objects.dummy import Dummy\n",
    "from pyrep.robots.end_effectors.panda_gripper import PandaGripper\n",
    "from rlbench.backend.scene import Scene\n",
    "from rlbench.backend.exceptions import *\n",
    "from rlbench.observation_config import ObservationConfig, CameraConfig\n",
    "from rlbench.backend.robot import Robot\n",
    "from rlbench.utils import name_to_task_class\n",
    "# from task_validator import task_smoke, TaskValidationError\n",
    "import shutil\n",
    "\n",
    "pr = PyRep()\n",
    "ttt_file = join('/home/pguhur/src/vln-robot/RLBench/rlbench', TTT_FILE)\n",
    "pr.launch(ttt_file, responsive_ui=True)\n",
    "pr.step_ui()\n",
    "\n",
    "robot = Robot(Panda(), PandaGripper())\n",
    "cam_config = CameraConfig(rgb=True, depth=False, mask=False,\n",
    "                          render_mode=RenderMode.OPENGL)\n",
    "obs_config = ObservationConfig()\n",
    "obs_config.set_all(False)\n",
    "obs_config.right_shoulder_camera = cam_config\n",
    "obs_config.left_shoulder_camera = cam_config\n",
    "obs_config.overhead_camera = cam_config\n",
    "obs_config.wrist_camera = cam_config\n",
    "obs_config.front_camera = cam_config\n",
    "\n",
    "scene = Scene(pr, robot, obs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6177c82-ea8c-4b0f-b62a-b5a4cd3701b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/tmp/rlbench_data/push_button/variation0/episodes/episode0/low_dim_obs.pkl', 'rb') as fid:\n",
    "    low_dim_obs = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e660434-09a6-4dc1-9dbd-226494234dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dim_obs[3].misc['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d45ac8c-3033-44a3-a915-baf1d021e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "glove_path = '/scratch/diffrac/glove.6B.50d.txt'\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open(glove_path, 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = torch.Tensor(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264b71c-fe64-44c2-af48-735f74361c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instr = 'Reach the blue ball'\n",
    "\n",
    "embeddings = torch.stack([embeddings_dict[w] for w in instr.lower().split()])\n",
    "embeddings = embeddings.unsqueeze(0)\n",
    "print(embeddings.shape)\n",
    "\n",
    "lstm = nn.LSTM(input_size=50, hidden_size=30, bidirectional=True, batch_first=True)\n",
    "output, (hn, cn) = lstm(embeddings)\n",
    "\n",
    "hn = einops.rearrange(hn, 'dn b h -> b (dn h)')\n",
    "print(hn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13492c9c-0894-4b88-88ca-049cc613967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "file = '/scratch/jeanzay/scratch/datasets/vlmbench-raw/train/stack_cubes_color/variation3/episodes/episode0/configs.pkl'\n",
    "with open(file, 'rb') as fid:\n",
    "    data = pkl.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d5e53-dcae-489a-bc5c-77411f663ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f5502-c04c-43e1-b8b1-466d4c4f5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.high_level_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62471aa8-5581-4103-861a-abf2dffd88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.task_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71a475-026a-4eaf-bad0-e53e8b4bda3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VLMBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d13cb-4661-4b08-bda8-052fd69dd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amsolver.backend.scene import Scene\n",
    "from amsolver.backend.robot import Robot\n",
    "from amsolver.utils import name_to_task_class\n",
    "from amsolver.backend.const import TTT_FILE\n",
    "from amsolver.observation_config import ObservationConfig, CameraConfig\n",
    "from pyrep.robots.end_effectors.panda_gripper import PandaGripper\n",
    "from pyrep.robots.arms.panda import Panda\n",
    "from pyrep import PyRep\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef758cf1-170f-4847-b43f-f816a6bc1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PyRep()\n",
    "ttt_file = os.path.join('/home/pguhur/src/vln-robot/vlmbench/amsolver', TTT_FILE)\n",
    "pr.launch(ttt_file, responsive_ui=True)\n",
    "pr.step_ui()\n",
    "\n",
    "robot = Robot(Panda(), PandaGripper())\n",
    "obs_config = ObservationConfig()\n",
    "obs_config.set_all(False)\n",
    "\n",
    "scene = Scene(pr, robot, obs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e2ba6c-79f6-41b8-add0-68ba48ab8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [t.stem for t in Path('/home/pguhur/src/vln-robot/vlmbench/vlm/tasks/').glob('*.py')]\n",
    "\n",
    "root_dir = '/gpfsstore/rech/vuo/uok79zh/datasets/vlmbench/valid/unseen'\n",
    "\n",
    "tarfiles = {\n",
    "    'wipe_table': f'{root_dir}/wipe.tar.gz',\n",
    "    'drop_pen': f'{root_dir}/drop.tar.gz',\n",
    "    'stack_cubes': f'{root_dir}/stack.tar.gz',\n",
    "    'place_into_shape_sorter': f'{root_dir}/shape_sorter.tar.gz',\n",
    "    'pour_demo': f'{root_dir}/pour.tar.gz',\n",
    "    'open_door': f'{root_dir}/door.tar.gz',\n",
    "    'open_drawer': f'{root_dir}/drawer.tar.gz',\n",
    "    'pick_cube': f'{root_dir}/pick.tar.gz',\n",
    "}\n",
    "for task_name in tasks[1:]:\n",
    "    task_class = name_to_task_class(task_name,parent_folder='vlm')\n",
    "    task = task_class(pr, robot)\n",
    "    # scene.load(task, ttms_folder='./vlm/task_ttms')\n",
    "    task_stem = '_'.join(task_name.split('_')[:-1])\n",
    "    \n",
    "    if task_stem not in tarfiles:\n",
    "        # print(task_stem)\n",
    "        continue\n",
    "        \n",
    "    tarfile = tarfiles[task_stem]\n",
    "    \n",
    "    try:\n",
    "        var_count = task.variation_count()\n",
    "    except NotImplementedError:\n",
    "        continue\n",
    "        \n",
    "    for i in range(var_count):\n",
    "        print(f\"{task_name},{i},{tarfile}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28eac76-cc49-4d54-ab85-d61506419485",
   "metadata": {},
   "source": [
    "### VLMbench: keypoint detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5cf767-5f71-4e3a-875f-b8c4cf150302",
   "metadata": {},
   "outputs": [],
   "source": [
    "* use the front camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e90e6c-cf97-429b-865d-9b0d7a4ddadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.obs_config.front_camera.mask = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858e3d6-a943-4ab0-93dc-622c3d035d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demos = env.get_demo('drop_pen_color', 16, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a013683-f720-4394-93ff-a8775b91ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.apply_cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f0d07-73f6-4a2f-9872-316548e9de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../vlmbaseline\n",
    "\n",
    "import sys\n",
    "\n",
    "from utils import RLBenchEnv\n",
    "env = RLBenchEnv(\n",
    "            data_path='/scratch/jeanzay/scratch/datasets/vlmbench-raw/valid-seen',\n",
    "            apply_rgb=True,\n",
    "            apply_pc=True,\n",
    "            apply_cameras=(\"front\"),\n",
    "        )\n",
    "\n",
    "# Why is it not done from the constructor?\n",
    "env.obs_config.front_camera.rgb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bccc71-9710-45d2-9442-088a8f86454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "demos = env.get_demo('drop_pen_color', 16, 1)\n",
    "demo = demos[0]\n",
    "\n",
    "from utils import _is_stopped\n",
    "\n",
    "def keypoint_discovery(demo):\n",
    "    episode_keypoints = []\n",
    "    prev_gripper_open = demo[0].gripper_open\n",
    "    stopped_buffer = 0\n",
    "    for i, obs in enumerate(demo):\n",
    "        stopped = _is_stopped(demo, i, obs, stopped_buffer)\n",
    "        stopped_buffer = 4 if stopped else stopped_buffer - 1\n",
    "        # If change in gripper, or end of episode.\n",
    "        last = i == (len(demo) - 1)\n",
    "        if i > 5 and (obs.gripper_open != prev_gripper_open or last or stopped):\n",
    "            episode_keypoints.append(i)\n",
    "        prev_gripper_open = obs.gripper_open\n",
    "    if (\n",
    "        len(episode_keypoints) > 1\n",
    "        and (episode_keypoints[-1] - 1) == episode_keypoints[-2]\n",
    "    ):\n",
    "        episode_keypoints.pop(-2)\n",
    "\n",
    "    return episode_keypoints\n",
    "\n",
    "keypoint_discovery(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e1dac-8b55-4afb-bc4f-95cad8096650",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in demo[0].__dict__.keys():\n",
    "    if getattr(demo[0], k) is not None:\n",
    "        print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218217a8-e026-431a-b304-608230f1399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for k in keypoint_discovery(demo):\n",
    "    print(k)\n",
    "    # plt.imshow(\n",
    "    plt.imshow(demo[k].front_rgb)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
